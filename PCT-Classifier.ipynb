{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchmetrics as tm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from datasets import ModelNet40, ScanObjectNN\n",
    "from pooling import MA_Pooling, KNN_Pooling\n",
    "from embedding_modules import Naive_Embedding,  Neighboorhood_Embedding\n",
    "from Attention_modules.attention_blocks import Self_Attention_Block, MultiHead_Attention_Block, MultiHead_Attention_Block3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset, batch_size):\n",
    "    #Split into validation and test set\n",
    "    test_set = dataset(1024,partition='test')\n",
    "    validation_length = int(len(test_set)*0.5)\n",
    "    test_length = len(test_set) - validation_length\n",
    "    validation_set, test_set = random_split(test_set, [validation_length, test_length], generator=torch.Generator().manual_seed(42))\n",
    "    train_dataloader = DataLoader(dataset(1024,partition='train', data_augmentation=True), num_workers=8,\n",
    "                            batch_size=loader_batch, shuffle=True, drop_last=True)\n",
    "    val_dataloader   = DataLoader(validation_set, num_workers=8,\n",
    "                            batch_size=loader_batch, shuffle=False, drop_last=False)\n",
    "    test_dataloader  = DataLoader(test_set, num_workers=8,\n",
    "                            batch_size=loader_batch, shuffle=False, drop_last=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the original PCT classifier refered to as PCT on the blog-post\n",
    "class PCT_Classifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate=0.01, input_features=3, attention_layers=4, encoder_channels=256, \n",
    "                 key_size=0.25, value_size=1, pooling =\"both\",linear_encoder_layer=1024, \n",
    "                 classification_layer_size = 256 , dropout=0.5, \n",
    "                 naive_embedding=False, k=32, sampling=0.25,  positional_embedding=True):\n",
    "        \n",
    "        super(PCT_Classifier, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_features = input_features\n",
    "        self.attention_layers = attention_layers\n",
    "        self.encoder_channels = encoder_channels\n",
    "        self.key_size = key_size\n",
    "        self.value_size = value_size\n",
    "        self.linear_encoder_layer = linear_encoder_layer\n",
    "        self.pooling = pooling\n",
    "        self.classification_layer_size = classification_layer_size\n",
    "        self.dropout = dropout\n",
    "        self.k = k\n",
    "        self.sampling = sampling\n",
    "        self.naive_embedding = naive_embedding\n",
    "        self.positional_embedding = positional_embedding\n",
    "             \n",
    "        if naive_embedding == True:\n",
    "            self.embedding = Naive_Embedding(self.input_features, self.encoder_channels)\n",
    "        elif naive_embedding == False:\n",
    "            self.embedding = Neighboorhood_Embedding(self.input_features, self.encoder_channels,\n",
    "                                                 self.k ,self.sampling, self.positional_embedding)\n",
    "        #why the different sizes this is bizzare\n",
    "        self.attention_block = Self_Attention_Block(self.attention_layers, self.encoder_channels, self.key_size,\n",
    "                                                            self.value_size)\n",
    "        #Leaky Relu is here in the implementaiton by the authors\n",
    "        self.conv_fuse = nn.Sequential(nn.Conv1d(self.encoder_channels*(self.attention_layers+1), self.linear_encoder_layer, kernel_size=1, bias=False),\n",
    "                                        nn.BatchNorm1d(self.linear_encoder_layer),\n",
    "                                        nn.LeakyReLU(0.02))\n",
    "        \n",
    "        assert self.pooling in {\"both\", \"max\", \"avg\"}, \"Pooling must be either max, avg or both\" \n",
    "        self.ma_pooling = MA_Pooling(self.pooling)\n",
    "        if pooling == \"both\":\n",
    "            self.classification_input = self.linear_encoder_layer*2\n",
    "        else:\n",
    "            self.classification_input = self.linear_encoder_layer\n",
    "            \n",
    "        self.classification_layers = nn.Sequential(\n",
    "            nn.Linear(self.classification_input, self.classification_layer_size*2, bias=False),\n",
    "            nn.BatchNorm1d(self.classification_layer_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout),\n",
    "            nn.Linear(self.classification_layer_size*2, self.classification_layer_size, bias=False),\n",
    "            nn.BatchNorm1d(self.classification_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout),\n",
    "            nn.Linear(self.classification_layer_size, self.num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, coordinates = self.embedding(x)\n",
    "        attention = self.attention_block(x, coordinates)\n",
    "        x = torch.cat((x, attention), dim=1)\n",
    "        x = self.conv_fuse(x)\n",
    "        x = self.ma_pooling(x)\n",
    "        x = self.classification_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y) #.squeeze(1)\n",
    "        accuracy = tm.functional.accuracy(logits, y)\n",
    "        self.log(\"loss\", loss, on_epoch=True, prog_bar=True)                         \n",
    "        self.log(\"accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y) #.squeeze(1)\n",
    "        accuracy = tm.functional.accuracy(logits, y)\n",
    "        self.log(\"val_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True) \n",
    "               \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        sheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 75)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": sheduler, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using MHA within a more traditional classifier refered to as MHA-PCT in the paper:\n",
    "class PCT_Classifier_2(pl.LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate=0.01, input_features=3, attention = \"self\", encoder_channels=256, \n",
    "                 attention_layers=4, heads=8, forward_expansion=4, attention_dropout=0.1,\n",
    "                 key_size=0.25, value_size=1, pooling =\"both\", classification_layer_size = 256 , dropout=0.5, \n",
    "                 embedding=\"neighboorhood\", k=32, sampling=0.25, positional_embedding=False):\n",
    "        #add positional-embedding option\n",
    "        #think about skip before the end\n",
    "        #maybe a big layer could actually be good\n",
    "        super(PCT_Classifier_2, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_features = input_features\n",
    "        self.attention = attention\n",
    "        self.encoder_channels = encoder_channels\n",
    "        self.attention_layers = attention_layers\n",
    "        self.heads = heads\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.key_size = key_size\n",
    "        self.value_size = value_size\n",
    "        self.pooling = pooling\n",
    "        self.classification_layer_size = classification_layer_size\n",
    "        self.dropout = dropout\n",
    "        self.k = k\n",
    "        self.sampling = sampling\n",
    "        self.embedding = embedding\n",
    "        self.positional_embedding = positional_embedding\n",
    "             \n",
    "        if embedding == \"naive\":\n",
    "            self.embedding = Naive_Embedding(self.input_features, self.encoder_channels)\n",
    "        elif embedding == \"neighboorhood\":\n",
    "            self.embedding = Neighboorhood_Embedding(self.input_features, self.encoder_channels,\n",
    "                                                 self.k ,self.sampling, self.positional_embedding)\n",
    "      \n",
    "        else:\n",
    "            raise ValueError(\"embedding must be either naive, neighboorhood)\n",
    "        \n",
    "        if self.attention == \"self\":\n",
    "            self.attention_block = Self_Attention_Block(self.attention_layers, self.encoder_channels, self.key_size,\n",
    "                                                            self.value_size)\n",
    "        elif self.attention ==\"multihead\":\n",
    "            self.attention_block = MultiHead_Attention_Block(self.attention_layers, self.heads, self.encoder_channels,\n",
    "                                                             self.forward_expansion, self.attention_dropout)\n",
    "        else:\n",
    "            raise ValueError(\"attention must be either self or multihead\")\n",
    "        \n",
    "    \n",
    "        if self.pooling in {\"both\", \"max\", \"avg\"}:\n",
    "            self.pooling = MA_Pooling(self.pooling)\n",
    "        \n",
    "        if pooling == \"both\":\n",
    "            self.classification_input = self.encoder_channels*2*(self.attention_layers)\n",
    "        else:\n",
    "            self.classification_input = self.encoder_channels*(self.attention_layers)\n",
    "        \n",
    "    \n",
    "        self.classification_layers = nn.Sequential(\n",
    "            nn.Linear(self.classification_input, self.classification_layer_size*2, bias=False),\n",
    "            nn.BatchNorm1d(self.classification_layer_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout),\n",
    "            nn.Linear(self.classification_layer_size*2, self.classification_layer_size, bias=False),\n",
    "            nn.BatchNorm1d(self.classification_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout),\n",
    "            nn.Linear(self.classification_layer_size, self.num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, coordinates = self.embedding(x)\n",
    "        if self.positional_embedding == True:\n",
    "            x = self.attention_block(x, coordinates)\n",
    "        else:\n",
    "            x = self.attention_block(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.classification_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)#.squeeze(1)\n",
    "        accuracy = tm.functional.accuracy(logits, y)\n",
    "        self.log(\"loss\", loss, on_epoch=True, prog_bar=True)                         \n",
    "        self.log(\"accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        #Try Label smoothing doesn't work that well here\n",
    "        loss = F.cross_entropy(logits, y)#.squeeze(1)\n",
    "        accuracy = tm.functional.accuracy(logits, y)\n",
    "        self.log(\"val_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True) \n",
    "               \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        #sheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 250)\n",
    "        sheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": sheduler, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer with KNN-pooling and a relatively different structure to the PCT refered to as PCT_MHA 2\n",
    "class PCT_Classifier_4(pl.LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate=0.01, input_features=3, encoder_channels=256, \n",
    "                 attention_layers=4, heads=8, attention_dropout=0.1, classification_layer_size = 256 , dropout=0.5, \n",
    "                 embedding=\"neighboorhood\", k=32, sampling=0.25, positional_embedding=False, pool_points=8):\n",
    "        #add positional-embedding option\n",
    "        #think about skip before the end\n",
    "        #maybe a big layer could actually be good\n",
    "        super(PCT_Classifier_4, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_features = input_features\n",
    "        self.encoder_channels = encoder_channels\n",
    "        self.attention_layers = attention_layers\n",
    "        self.heads = heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.classification_layer_size = classification_layer_size\n",
    "        self.dropout = dropout\n",
    "        self.k = k\n",
    "        self.sampling = sampling\n",
    "        self.embedding = embedding\n",
    "     \n",
    "             \n",
    "        if embedding == \"naive\":\n",
    "            self.embedding = Naive_Embedding(self.input_features, self.encoder_channels)\n",
    "        elif embedding == \"neighboorhood\":\n",
    "            self.embedding = Neighboorhood_Embedding(self.input_features, self.encoder_channels,\n",
    "                                                 self.k ,self.sampling, False)\n",
    "        \n",
    "        self.attention_block = MultiHead_Attention_Block3(self.attention_layers, self.heads, self.encoder_channels, self.attention_dropout)\n",
    "\n",
    "        self.conv_fuse = nn.Sequential(nn.Conv1d(self.encoder_channels, self.encoder_channels*2, kernel_size=1, bias=False),\n",
    "                                            nn.BatchNorm1d(self.encoder_channels*2),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Conv1d(self.encoder_channels*2, self.encoder_channels, kernel_size=1, bias=False),\n",
    "                                            nn.BatchNorm1d(self.encoder_channels),\n",
    "                                            nn.ReLU()\n",
    "                                          )\n",
    "        self.pooling = KNN_Pooling(pool_points, 32)\n",
    "        \n",
    "        self.classification_input =(pool_points+1)*self.encoder_channels\n",
    "        \n",
    "        \n",
    "        self.classification_layers = nn.Sequential(\n",
    "            nn.Linear(self.classification_input, self.classification_layer_size*2, bias=False),\n",
    "            nn.BatchNorm1d(self.classification_layer_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout),\n",
    "            nn.Linear(self.classification_layer_size*2, self.classification_layer_size, bias=False),\n",
    "            nn.BatchNorm1d(self.classification_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout),\n",
    "            nn.Linear(self.classification_layer_size, self.num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, coordinates = self.embedding(x)\n",
    "        x = self.attention_block(x)\n",
    "        x =  self.conv_fuse(x)\n",
    "        global_feature  = torch.max(x,2)[0]\n",
    "        x = self.pooling(x, coordinates)\n",
    "        x = torch.cat((global_feature, x), 1)\n",
    "        x = self.classification_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)#.squeeze(1)\n",
    "        accuracy = tm.functional.accuracy(logits, y)\n",
    "        self.log(\"loss\", loss, on_epoch=True, prog_bar=True)                         \n",
    "        self.log(\"accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        #Try Label smoothing doesn't work that well here\n",
    "        loss = F.cross_entropy(logits, y)#.squeeze(1)\n",
    "        accuracy = tm.functional.accuracy(logits, y)\n",
    "        self.log(\"val_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True) \n",
    "               \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        sheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 250)\n",
    "        #heduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": sheduler, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "checkpoint_callback_accuracy = pl.callbacks.ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\",save_top_k=1)\n",
    "checkpoint_callback_loss = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode=\"min\",save_top_k=1)\n",
    "    \n",
    "early_stopping_callback = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    \n",
    "    \n",
    "\n",
    "trainer = pl.Trainer(overfit_batches= 0, gpus=-1, benchmark=True, max_epochs=250,\n",
    "                     callbacks=[checkpoint_callback_accuracy, checkpoint_callback_loss,early_stopping_callback],\n",
    "                    logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of training with the orignal pct and Modelnet40\n",
    "train_dataloader, val_dataloader, test_dataloader= create_datasets(Modelnet40, 32)\n",
    "model = PCT_Classifier(40, dropout=0.5, \n",
    "                            pooling=\"max\",\n",
    "                            learning_rate=0.0001,\n",
    "                            key_size=0.25,\n",
    "                            k=32,\n",
    "                            sampling=0.25,\n",
    "                            encoder_channels=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate the results\n",
    "trainer.validate(dataloaders=val_dataloader, ckpt_path=checkpoint_callback_accuracy.best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
